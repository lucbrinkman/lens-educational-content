---
id: 01f6df31-099f-48ed-adef-773cc4f947e4
---
### Article: AI Is Grown, Not Built
source:: [[]] https://archive.is/blU6r

#### Text
content::
We don’t "build" intelligence brick by brick; instead, we create conditions for it to develop using massive datasets. This "growth" leads to emergent properties, skills that the system wasn't explicitly taught and that often surprise the creators themselves. The first material in this module examines these features of the current AI development paradigm and explores the consequences of such an approach.

#### Article-excerpt
to:: "and we’re already seeing the warning signs."

#### Text
content::
`<prompt the user on how they should interact with the chatbot>`{>>Example: In your own words, what is instrumental convergence?<<}
#### Chat: Discussion on X-Risk
instructions::
TLDR of what the user just read:{>>Example: 
AI x-risk is the hypothesis that AGI/superintelligence could cause human extinction
or irreversible collapse. The risk combines: capability advantages, recursive
self-improvement, and emerging dangerous capabilities (manipulation, cyberattacks,
bioweapons)—amplified by competitive "race to the bottom" dynamics. The core crux
is alignment: specifying goals, ensuring corrigibility, handling instrumental
convergence. Intelligence and values are orthogonal—moral behavior isn't automatic.<<}
The authors argue that modern AI poses a fundamental control problem: AI systems are "grown" through gradient descent rather than designed, meaning engineers understand the training process but not the resulting system. This creates a dangerous gap between capability and control.

The core logic proceeds as follows:

- The training process is opaque: Trillions of parameters are tuned automatically through gradient descent, producing conversational ability without human comprehension of how those numbers yield behavior.
- Analogy to biology: Engineers' relationship to AI is like biologists' to DNA—they can see the components but cannot predict emergent behaviors without running the system.
- Evidence of unintended behaviors: Examples like Grok's "MechaHitler" incident demonstrate that even well-resourced companies cannot reliably control AI outputs, despite Musk spending hours trying.
- Expert consensus on interpretability failure: Leaders from OpenAI, Anthropic, and DeepMind acknowledge they don't understand how their systems work.
- The escalating danger: As companies race toward superintelligence—AI surpassing humans at all mental tasks—this lack of control becomes catastrophic, since "weird drives and behaviors get trained into them, for reasons nobody entirely understands."

The authors conclude this trajectory requires stopping through international treaty.

Discussion topics to explore:{>>Examples:
- What is "instrumental convergence"? Why would any smart AI seek self-preservation and resources?
- What is the "Gorilla Problem" (Stuart Russell's analogy)?
- How do "Monkey's Paw" or "King Midas" effects apply to goal specification?
- What do skeptics like Yann LeCun argue, and what are the counter-arguments?
- Why might "kill switches" fail against superintelligence?<<}
- The authors claim engineers understand the training process but not the resulting AI. Is this fundamentally different from traditional software, where a programmer might understand each line of code but struggle to predict complex emergent behaviors in a large system? Where exactly does the "understanding gap" become qualitatively different?
- The Grok "MechaHitler" incident is presented as evidence that even experts cannot control grown systems. What would count as counter-evidence? If a company successfully prevented such behaviors, would that prove grown systems can be controlled, or just that they got lucky this time?
- If we accept that grown systems are fundamentally less understandable than engineered ones, what does this mean for safety approaches? Can we have confidence in systems we don't understand, or does the grown nature of AI require entirely different safety paradigms than traditional engineering?

Ask what they found surprising or new. Check if they can explain `<key concept>` in their own words—it's a key concept.

`<tell the chatbot what prompt the user is responding to`{>>Example: The user has just answered the following question: "In your own words, what is instrumental convergence?"<<}