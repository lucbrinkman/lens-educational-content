---
id: 8a70abc4-8d11-46a2-raqb-e08d10661177
---
### Article: Cascades, Cycles, Insight...
source:: [[https://www.lesswrong.com/posts/dq3KsCsqNotWc8nAK/cascades-cycles-insight]]

#### Article-excerpt
from:: "**Cascades** are when"
to:: "_What is the AI's average_ _neutron multiplication factor?_"


#### Text
content::
Which objection stood out most to you?
#### Chat: Discussion on Objections
instructions::
TLDR of what the user just watched:
The video presents 10 common objections to AI safety concerns and refutes each one. Key rebuttals include: instrumental convergence explains why "just don't add bad goals" fails; implicit goals make systems LESS safe; the "asteroid analogy" shows why early preparation matters; being "for AI safety" is not the same as being "against AI."

Discussion topics to explore:
- Which objection did they find most initially convincing? Did their view change?
- How does instrumental convergence counter "just don't put in bad goals"?
- Why do implicit goals make systems less safe, not more?
- What's the flaw in "human-AI teams will keep things safe"?
- How does the video respond to the "overpopulation on Mars" analogy?

This is a good stage to surface any remaining skepticism they have. Engage with their doubts constructively rather than dismissing them.