---
slug: introduction-to-ai-safety
title: Introduction to AI Safety
---
# Video: A.I. - Humanity's Final Invention
source:: [[video_transcripts/kurzgesagt-ai-humanitys-final-invention]]

## Text
content::
!# Welcome to AI Safetys

We begin by examining the potential of AI and the risks and opportunities
that the characteristics of this technology present to humanity.

Watch this video from Kurzgesagt to understand why artificial intelligence
might be humanity's most important invention.

## Video-excerpt
from:: 0:00
to:: 5:00

## Text
content::
**Reflection:**

The video describes how humans dominate Earth because of our general
intelligence. It also explains the difference between narrow AI and AGI. In your own words, what is the difference between narrow and general intelligence?

## Chat
showUserPreviousContent:: true
instructions::
TLDR of what the user just watched:
Humans dominate Earth because general intelligence enabled cumulative knowledge.
Modern AI evolved from narrow tools into opaque "black box" learning systems.

Discussion topics to explore:
- What is intelligence as "problem-solving ability" and why is it a source of power?
- Why are neural networks called "black boxes"?
- What's the difference between narrow AI (like ChatGPT) and AGI?

Start by asking what stood out or surprised them. Use Socratic questioning to
check their understanding of these concepts.

## Video-excerpt
from:: 5:00
to:: 10:00

## Text
content::
The video introduces the concept of an **intelligence explosion** - a rapid,
recursive cycle of AI self-improvement that could outpace human oversight.

## Chat
showUserPreviousContent:: true
showTutorPreviousContent:: true
instructions::
The user just watched the second half of the video about AI risks.

Discussion topics:
- How could an "intelligence explosion" happen through recursive self-improvement?
- Why might controlling superintelligent AI be difficult?

Check if they understand why speed of improvement matters.

# Article: Existential Risk from AI
source:: [[articles/wikipedia-existential-risk-from-ai]]

## Text
content::
!## Understanding Existential Risk from AI

Now let's read an overview of the main concepts regarding AI as a source
of existential threat: what capabilities of this technology are considered
most concerning, and why the task of eliminating AI risks differs from
similar tasks for other technologies.

## Article-excerpt
from:: "**Existential risk from artificial intelligence**"
to:: "improve their fundamental architecture."

## Text
content::
**Key concepts so far:**

- AI x-risk refers to the possibility that AGI could cause human extinction
- The "Gorilla Problem": just as gorillas depend on human goodwill, humans
  might depend on AI's goodwill
- Many leading AI researchers take this risk seriously

## Chat
showUserPreviousContent:: true
instructions::
The user just read the introduction to AI existential risk from Wikipedia.

Key concepts covered:
- AI x-risk hypothesis
- The gorilla analogy (Stuart Russell)
- Expert surveys showing concern

Discussion topics:
- What is the "Gorilla Problem" and why is it a useful analogy?
- Why do many AI researchers believe there's a significant chance of catastrophe?

Ask what they found surprising or new.

## Article-excerpt
from:: "One of the earliest authors"
to:: "strong public buy-in."

## Article-excerpt
from:: "### General Intelligence"
to:: "evasion of human control"

## Text
content::
**The path from AGI to superintelligence:**

Notice Bostrom's list of advantages AI has over human brains: speed,
scalability, duplicability. These aren't science fiction - they're
inherent properties of software.

## Chat
instructions::
The user just read about AGI and superintelligence capabilities.

Key points:
- AGI = human-level across most tasks
- Superintelligence = vastly exceeds humans in all domains
- AI advantages: speed, scalability, duplicability, editability

Discussion topics:
- Why might the transition from AGI to superintelligence be rapid?
- Which of the AI advantages over human brains seems most significant?

# Text: Summary
content::
!# Key Takeaways

1. **Intelligence is power** - Humans dominate Earth because of general intelligence
2. **AI is different** - Digital minds can run faster, scale, and be copied
3. **The alignment problem** - Ensuring AI goals match human values is extremely difficult
4. **Expert concern** - Many leading researchers believe AI poses existential risk
5. **Timelines are uncertain** - AGI could arrive within years or decades

In the next lesson, we'll explore common objections to AI safety concerns
and how researchers respond to them.
