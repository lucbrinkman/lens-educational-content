---
slug: introduction
title: Introduction
---
# Text
content::
We begin by examining the potential of AI and the risks and opportunities that the characteristics of this technology present to humanity. 

# [[../Sections/A.I. - Humanity's Final Invention]]

# [[wikip]]



# Video: 10 Reasons to Ignore AI Safety
source:: [[../video_transcripts/robertmiles-10-reasons-to-ignore-ai-safety]]

## Video-excerpt
from:: 0:00
to:: 14:48

## Text
content::
Which objection stood out most to you?
## Chat: Discussion on Objections
instructions::
TLDR of what the user just watched:
The video presents 10 common objections to AI safety concerns and refutes each one. Key rebuttals include: instrumental convergence explains why "just don't add bad goals" fails; implicit goals make systems LESS safe; the "asteroid analogy" shows why early preparation matters; being "for AI safety" is not the same as being "against AI."

Discussion topics to explore:
- Which objection did they find most initially convincing? Did their view change?
- How does instrumental convergence counter "just don't put in bad goals"?
- Why do implicit goals make systems less safe, not more?
- What's the flaw in "human-AI teams will keep things safe"?
- How does the video respond to the "overpopulation on Mars" analogy?

This is a good stage to surface any remaining skepticism they have. Engage with their doubts constructively rather than dismissing them.

# Article: Four Background Claims (Optional)
source:: [[../articles/nate-soares-four-background-claims]]
optional:: true

## Text
content::
This text explains exactly how the emergence of AI smarter than humans could become an event with enormous stakes, and why, in the author's opinion, there is already meaningful work being done today that increases the chance of a positive outcome. The author identifies four key premises that underpin his entire perspective on the prospects of AI.

## Article-excerpt

# Article: Worst-Case Thinking (Optional)
source:: [[../articles/buck-worst-case-thinking-in-ai-alignment]]
optional:: true

## Text
content::
In discussions of AI safety, people often propose the assumption that something will go as badly as possible. Different people may do this for different reasons; in this essay, the author reviews some of the most common reasons and writes about how this difference might manifest itself and what it means.

## Article-excerpt
