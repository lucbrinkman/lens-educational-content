---
slug: narrative-test
title: Introduction to AI Safety
---

# Text
content::
!# Welcome to AI Safety

We begin by examining the potential of AI and the risks and opportunities
that the characteristics of this technology present to humanity.
# Video: A.I. - Humanity's Final Invention
source:: [[video_transcripts/kurzgesagt-ai-humanitys-final-invention]]

## Text
content::
Watch this video from Kurzgesagt to understand why artificial intelligence
might be humanity's most important invention.

## Video-excerpt
from:: 0:00
to:: 5:00

## Text
content::
**Reflection:**

The video describes how humans dominate Earth because of our general
intelligence. It also explains the difference between narrow AI and AGI.

## Chat
showUserPreviousContent:: true
showTutorPreviousContent:: true
instructions::
TLDR of what the user just watched:
Humans dominate Earth because general intelligence enabled cumulative knowledge.
Modern AI evolved from narrow tools into opaque "black box" learning systems.
AGI matters because digital minds can run faster, scale, and be copied—potentially
outcompeting humans and concentrating power.

Discussion topics to explore:
- What is intelligence as "problem-solving ability" and why is it a source of power?
- Why are neural networks called "black boxes"?
- What's the difference between narrow AI (like ChatGPT) and AGI?

Start by asking what stood out or surprised them. Use Socratic questioning to
check their understanding of these concepts. Don't lecture—help them articulate
their own thinking.

## Video-excerpt
from:: 5:00
to:: 10:00

## Text
content::
The video introduces the concept of an **intelligence explosion** - a rapid,
recursive cycle of AI self-improvement that could outpace human oversight.

## Chat
showUserPreviousContent:: true
showTutorPreviousContent:: true
instructions::
The user just watched the second half of the video about AI risks and intelligence explosion.

Discussion topics:
- How could an "intelligence explosion" happen through recursive self-improvement?
- Why might controlling superintelligent AI be difficult?
- What does it mean that "the fate of humanity could depend on machine superintelligence"?

Check if they understand why speed of improvement matters. Ask them to explain
the intelligence explosion concept in their own words.

# Article: Understanding Existential Risk from AI
source:: [[articles/wikipedia-existential-risk-from-ai]]

## Text
content::
!# Understanding Existential Risk from AI

Now let's read an overview of the main concepts regarding AI as a source
of existential threat: what capabilities of this technology are considered
most concerning, and why the task of eliminating AI risks differs from
similar tasks for other technologies.

## Article-excerpt
from:: "**Existential risk from artificial intelligence**"
to:: "improve their fundamental architecture."

## Text
content::
**Key concepts so far:**

- AI x-risk refers to the possibility that AGI could cause human extinction
- The "Gorilla Problem": just as gorillas depend on human goodwill, humans
  might depend on AI's goodwill
- Many leading AI researchers take this risk seriously

## Chat
showUserPreviousContent:: true
showTutorPreviousContent:: true
instructions::
The user just read the introduction to AI existential risk from Wikipedia.

Key concepts covered:
- AI x-risk hypothesis
- The gorilla analogy (Stuart Russell)
- Expert surveys showing concern
- The alignment problem

Discussion topics:
- What is the "Gorilla Problem" and why is it a useful analogy?
- Why do many AI researchers believe there's a significant chance of catastrophe?
- What are the two main sources of concern (control and alignment)?

Ask what they found surprising or new. Check if they can explain the gorilla
analogy in their own words—it's a key concept.

## Article-excerpt
from:: "One of the earliest authors"
to:: "strong public buy-in."

## Text
content::
This history shows that concerns about AI risk aren't new - thinkers from
Alan Turing to Stephen Hawking have considered these questions.

## Article-excerpt
from:: "### General Intelligence"
to:: "evasion of human control"

## Text
content::
**The path from AGI to superintelligence:**

Notice Bostrom's list of advantages AI has over human brains: speed,
scalability, duplicability. These aren't science fiction - they're
inherent properties of software.

## Chat
showUserPreviousContent:: true
showTutorPreviousContent:: true
instructions::
The user just read about AGI and superintelligence capabilities.

Key points:
- AGI = human-level across most tasks
- Superintelligence = vastly exceeds humans in all domains
- AI advantages: speed, scalability, duplicability, editability
- Timelines are uncertain but potentially soon

Discussion topics:
- Why might the transition from AGI to superintelligence be rapid?
- Which of the AI advantages over human brains seems most significant?
- How does duplicability change the dynamics of AI development?

Help them understand why these technical properties have such large implications.

# Text: Summary
content::
!# Summary

**Key takeaways from this lesson:**

1. **Intelligence is power** - Humans dominate Earth because of general intelligence
2. **AI is different** - Digital minds can run faster, scale, and be copied
3. **The alignment problem** - Ensuring AI goals match human values is extremely difficult
4. **Expert concern** - Many leading researchers believe AI poses existential risk
5. **Timelines are uncertain** - AGI could arrive within years or decades

In the next lesson, we'll explore common objections to AI safety concerns
and how researchers respond to them.
